import requests
import os
import random
import hashlib
import concurrent.futures
from PIL import Image

# üîß –ù–∞—Å—Ç—Ä–æ–π–∫–∏
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/110.0.0.0 Safari/537.36"
}

SUBREDDITS = ["memes", "dankmemes", "funny", "wholesomememes", "PrequelMemes", "terriblefacebookmemes", 
              "cars", "carporn", "Shitty_Car_Mods", "CarMemes"]  # –ê–≤—Ç–æ, –º–µ–º–Ω—ã–µ —Å–∞–±—Ä–µ–¥–¥–∏—Ç—ã

BASE_FOLDER = "Memes"
HASHES_FILE = "downloaded_hashes.txt"


def get_memes(query, limit=5, min_upvotes=100):
    """–ò—â–µ—Ç –º–µ–º—ã –ø–æ –∑–∞–ø—Ä–æ—Å—É (–µ—Å–ª–∏ –Ω–µ –Ω–∞—Ö–æ–¥–∏—Ç, –±–µ—Ä–µ—Ç –≥–æ—Ä—è—á–∏–µ –ø–æ—Å—Ç—ã)"""
    all_images = []

    for subreddit in SUBREDDITS:
        print(f"üîç –ü–æ–∏—Å–∫ –º–µ–º–æ–≤ –≤ —Å–∞–±—Ä–µ–¥–¥–∏—Ç–µ: {subreddit}")

        # 1Ô∏è‚É£ –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–±—É–µ–º –ø–æ–∏—Å–∫ –ø–æ –∑–∞–ø—Ä–æ—Å—É
        search_url = f"https://www.reddit.com/r/{subreddit}/search.json?q={query}&restrict_sr=1&sort=relevance"
        response = requests.get(search_url, headers=HEADERS, timeout=5)
        
        if response.status_code == 200:
            data = response.json()
            posts = data.get("data", {}).get("children", [])

            for post in posts:
                title = post["data"].get("title", "").lower()
                upvotes = post["data"].get("ups", 0)
                image_url = post["data"].get("url")

                # –§–∏–ª—å—Ç—Ä—É–µ–º –ø–æ –∫–ª—é—á–µ–≤–æ–º—É —Å–ª–æ–≤—É –≤ –∑–∞–≥–æ–ª–æ–≤–∫–µ
                if query.lower() in title and image_url and (".jpg" in image_url or ".png" in image_url or ".jpeg" in image_url):
                    if upvotes >= min_upvotes:
                        all_images.append(image_url)
                        if len(all_images) >= limit:
                            return all_images  # –î–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –º–µ–º–æ–≤, –≤—ã—Ö–æ–¥–∏–º

        # 2Ô∏è‚É£ –ï—Å–ª–∏ –Ω–∏—á–µ–≥–æ –Ω–µ –Ω–∞—à–ª–∏, –ø—Ä–æ–±—É–µ–º –ø—Ä–æ—Å—Ç–æ —Ç–æ–ø–æ–≤—ã–µ –ø–æ—Å—Ç—ã
        hot_url = f"https://www.reddit.com/r/{subreddit}/hot.json?limit=25"
        response = requests.get(hot_url, headers=HEADERS, timeout=5)
        
        if response.status_code == 200:
            data = response.json()
            posts = data.get("data", {}).get("children", [])

            for post in posts:
                upvotes = post["data"].get("ups", 0)
                image_url = post["data"].get("url")

                if image_url and (".jpg" in image_url or ".png" in image_url):
                    if upvotes >= min_upvotes:
                        all_images.append(image_url)
                        if len(all_images) >= limit:
                            return all_images  # –î–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –º–µ–º–æ–≤, –≤—ã—Ö–æ–¥–∏–º

    return all_images


def get_image_hash(image_data):
    """–°–æ–∑–¥–∞–µ—Ç —Ö–µ—à –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤"""
    return hashlib.md5(image_data).hexdigest()


def load_existing_hashes():
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Ö–µ—à–∏ —É–∂–µ —Å–∫–∞—á–∞–Ω–Ω—ã—Ö –º–µ–º–æ–≤"""
    if not os.path.exists(HASHES_FILE):
        return set()
    with open(HASHES_FILE, "r") as f:
        return set(f.read().splitlines())


def save_new_hash(image_hash):
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –Ω–æ–≤—ã–π —Ö–µ—à –≤ —Ñ–∞–π–ª"""
    with open(HASHES_FILE, "a") as f:
        f.write(image_hash + "\n")


def save_image(img_url, category):
    """–°–∫–∞—á–∏–≤–∞–µ—Ç –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –≤ –Ω—É–∂–Ω—É—é –ø–∞–ø–∫—É"""
    category_folder = os.path.join(BASE_FOLDER, category.capitalize())  # –ü–∞–ø–∫–∞ —Å –∫–∞—Ç–µ–≥–æ—Ä–∏–µ–π
    os.makedirs(category_folder, exist_ok=True)  # –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫—É, –µ—Å–ª–∏ –µ—ë –Ω–µ—Ç

    img_data = requests.get(img_url).content
    img_hash = get_image_hash(img_data)

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ —Å–∫–∞—á–∏–≤–∞–ª–∏ –ª–∏ –º—ã —É–∂–µ —ç—Ç–æ—Ç –º–µ–º
    existing_hashes = load_existing_hashes()
    if img_hash in existing_hashes:
        print(f"‚ö†Ô∏è –î—É–±–ª–∏–∫–∞—Ç –Ω–∞–π–¥–µ–Ω, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º {img_url}")
        return None

    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∏–º—è —Ñ–∞–π–ª–∞
    file_name = os.path.join(category_folder, f"meme_{random.randint(1000, 9999)}.jpg")
    with open(file_name, "wb") as f:
        f.write(img_data)

    save_new_hash(img_hash)  # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ö–µ—à –Ω–æ–≤–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
    print(f"‚úÖ –ú–µ–º —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {file_name}")
    return file_name


def save_images_multithread(image_urls, category):
    """–°–∫–∞—á–∏–≤–∞–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤ –Ω–µ—Å–∫–æ–ª—å–∫–æ –ø–æ—Ç–æ–∫–æ–≤"""
    with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:
        futures = [executor.submit(save_image, img_url, category) for img_url in image_urls]
        saved_files = [f.result() for f in concurrent.futures.as_completed(futures)]
    return [file for file in saved_files if file]


def open_images(files):
    """–ê–≤—Ç–æ–æ—Ç–∫—Ä—ã—Ç–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π"""
    for file in files:
        if file:
            img = Image.open(file)
            img.show()


# üî• –ó–∞–ø—É—Å–∫
search_query = input("–í–≤–µ–¥–∏—Ç–µ —Ç–µ–º—É –¥–ª—è –ø–æ–∏—Å–∫–∞ –º–µ–º–æ–≤: ")
meme_count = int(input("–°–∫–æ–ª—å–∫–æ –º–µ–º–æ–≤ —Å–∫–∞—á–∞—Ç—å? (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 5): ") or 5)

memes = get_memes(search_query, limit=meme_count)

if memes:
    saved_files = save_images_multithread(memes, search_query)
    open_images(saved_files)  # –ê–≤—Ç–æ–æ—Ç–∫—Ä—ã—Ç–∏–µ –º–µ–º–æ–≤
else:
    print("üö´ –ú–µ–º–æ–≤ –ø–æ –∑–∞–ø—Ä–æ—Å—É –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –≤–æ –≤—Å–µ—Ö —Å–∞–±—Ä–µ–¥–¥–∏—Ç–∞—Ö.")
